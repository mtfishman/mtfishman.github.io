<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/basic.css"> <link rel=icon  href="/assets/favicon.png"> <link rel=stylesheet  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"> <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css"> <title>Projects</title> <header> <div class=blog-name ><a href="/">Matthew Fishman</a></div> <nav> <ul> <li><a href="/">Introduction</a> <li><a href="/about/">About Me</a> <li><a href="/projects/">Projects</a> <li><a href="/blog/">Blog</a> </ul> <img src="/assets/hamburger.svg" id=menu-icon > </nav> </header> <div class=franklin-content > <h1 id=projects ><a href="#projects" class=header-anchor >Projects</a></h1> <div class=franklin-toc ><ol><li><a href="#itensorsjl">ITensors.jl</a><ol><li><a href="#ndtensorsjl">NDTensors.jl</a><li><a href="#itensorvisualizationbasejl">ITensorVisualizationBase.jl</a><li><a href="#itensorglmakiejl">ITensorGLMakie.jl</a><li><a href="#itensorunicodeplotsjl">ITensorUnicodePlots.jl</a><li><a href="#itensorgaussianmpsjl">ITensorGaussianMPS.jl</a><li><a href="#itensorinfinitempsjl">ITensorInfiniteMPS.jl</a><li><a href="#itensorgpujl">ITensorGPU.jl</a><li><a href="#itensornetworkadjl">ITensorNetworkAD.jl</a><li><a href="#other_extensions_to_itensorsjl">Other extensions to ITensors.jl</a></ol><li><a href="#pastaqjl">PastaQ.jl</a><li><a href="#itensor_c">ITensor &#40;C&#43;&#43;&#41;</a><li><a href="#miscellaneous_julia_packages">Miscellaneous Julia Packages</a><ol><li><a href="#observersjl">Observers.jl</a><li><a href="#serializedelementarraysjl">SerializedElementArrays.jl</a></ol><li><a href="#tensor_network_algorithm_development">Tensor Network algorithm development</a><ol><li><a href="#free_fermion_tensor_networks">Free Fermion Tensor Networks</a><li><a href="#variational_uniform_matrix_product_states_and_tree_tensor_networks">Variational Uniform Matrix Product States and Tree Tensor Networks</a><li><a href="#improved_contraction_methods_for_infinite_2d_tensor_networks">Improved Contraction Methods for Infinite 2D Tensor Networks</a><li><a href="#easing_the_sign_problem_with_variational_circuits_and_automatic_differentation">Easing the Sign Problem with Variational Circuits and Automatic Differentation</a></ol><li><a href="#references">References</a></ol></div> <h1 id=coding_projects ><a href="#coding_projects" class=header-anchor >Coding Projects</a></h1> <h2 id=itensorsjl ><a href="#itensorsjl" class=header-anchor >ITensors.jl</a></h2> <p>I am the lead developer of <a href="https://github.com/ITensor/ITensors.jl">ITensors.jl <i class="fab fa-github"></i></a> , a full port of the <a href="https://github.com/ITensor/ITensor">C&#43;&#43; ITensor library <i class="fab fa-github"></i></a> to the Julia language. It is a library co-develop with <a href="http://itensor.org/miles">Miles Stoudenmire <i class="fas fa-external-link-alt"></i></a> for easily developing and running high performance tensor network calculations, with applications to quantum physics, quantum computing chemistry, and data science/machine learning.</p> <p><a href="https://itensor.org"><i class="fas fa-external-link-alt"></i> itensor.org</a></p> <p><a href="https://github.com/ITensor/ITensors.jl"><i class="fab fa-github"></i> Source code</a></p> <p><i class="fas fa-file-alt"></i> <span class=bibref ><a href="#fishman2020">Fishman et al. (2020)</a></span></p> <h2>ITensors.jl news</h2> <p><i class="fas fa-newspaper"></i> <strong>Automatic differentiation:</strong> Derivatives of basic tensor network operations using automatic differentiation are now supported. Reverse mode automatic differentiation &#40;AD&#41; primitives are defined using <a href="https://github.com/JuliaDiff/ChainRulesCore.jl">ChainRulesCore.jl <i class="fab fa-github"></i></a> , and derivatives can be computed with AD libraries like <a href="https://github.com/FluxML/Zygote.jl">Zygote.jl <i class="fab fa-github"></i></a> &#40;and eventually next generation AD libraries like <a href="https://github.com/JuliaDiff/Diffractor.jl">Diffractor.jl <i class="fab fa-github"></i></a> &#41;.</p> <p><i class="fas fa-newspaper"></i> <strong>Lazy Operator Algebra &#40;Ops&#41; system:</strong> Within ITensors.jl, we now have support for a general lazy operator algebra system called <a href="https://github.com/ITensor/ITensors.jl/tree/main/src/Ops">Ops <i class="fab fa-github"></i></a> . This system can be used to represent arbitrary sums, products, and other algebraic manipulations of quantum operators &#40;or more generally linear transformations in tensor product spaces&#41;. This can be used to represent local Hamiltonians and quantum circuits, as well as useful algebraic operations like expanding product of sums of operators and representing Trotter-Suzuki decompositions to transform exponentials of sums of operators into products of exponentials of operators. Additionally, operator representations can then be converted into explicit tensor representations for use in tensor network algorithms or to perform diagonalizations.</p> <p><i class="fas fa-tools"></i> The Ops systems needs testing, documentation, and automatic differentation support. Please <a href="/about/#online_presence">reach out</a> if you are interested in helping out&#33;</p> <h2>ITensors.jl wish list</h2> <p><i class="fas fa-tools"></i> <strong>Automatic differentiation:</strong> Our goal is to make the entire <code>ITensors.jl</code> package differentiable, but currently we only have basic operations like tensor contraction, addition, and index manipulation. More reverse mode automatic differentation rules &#40;<code>ChainRulesCore.jl</code> <code>rrule</code>s&#41; and/or modifications of internal <code>ITensors.jl</code> functions to avoid non-differentiable code patterns like mutation are needed to make this happen. The following functionality is high priority:</p> <ul> <li><p>Differentiation rules for tensor decompositions like SVD, QR, eigendecomposition, etc. This will allow us to more directly differentiate through approximate tensor network algorithms like CTMRG for automatic differentiation-based gradient optimization of infinite projected entangled pair states &#40;PEPS&#41;.</p> <li><p>Differentation rules for matrix product state &#40;MPS&#41; and matrix product operator &#40;MPO&#41; algebra operations, like contraction, addition, construction, etc.</p> <li><p>Differentation support for the new <a href="https://github.com/ITensor/ITensors.jl/tree/main/src/Ops">Ops system <i class="fab fa-github"></i></a>, which will allow differentiating through the construction of operators and Trotter decompositions. This will make for much more robust support for variational quantum circuit applications like quantum control &#40;though we already have initial support for that in <a href="https://github.com/GTorlai/PastaQ.jl">PastaQ.jl <i class="fab fa-github"></i></a>&#41;.</p> </ul> <p><i class="fas fa-tools"></i> <strong>More block sparse multithreading:</strong> We currently support block sparse multithreaded tensor contractions. We would like to add multithreading support to other block sparse operations, such as addition, permutation, and decomposition. Please <a href="/about/#online_presence">reach out</a> if you are interested in contributing code for that&#33;</p> <p><i class="fas fa-tools"></i> <strong>More block sparse factorizations:</strong> We currently support block sparse multithreaded tensor contractions. We would like to add multithreading support to other block sparse operations, such as addition, permutation, and decomposition. Please <a href="/about/#online_presence">reach out</a> if you are interested in contributing code for that&#33;</p> <p><i class="fas fa-tools"></i> <strong>ITensor slicing:</strong> We have good support for slicing dense tensors at the level of <a href="/projects/#ndtensorsjl">NDTensors.Tensor</a>, as well as slicing a single block of a block sparse <code>NDTensors.Tensor</code>. However, it would be helpful to have high level support for slicing at the level of ITensors, which requires having an interface for slicing Index objects, which could be represented by objects like <code>i &#61;&gt; 2:3</code> of type <code>Pair&#123;Index&#123;Int64&#125;,UnitRange&#123;Int64&#125;&#125;</code>. Additionally, it would be helpful to have more robust support for more general slices of block sparse tensors, including slices across multiple blocks and within blocks.</p> <hr /> <h3 id=ndtensorsjl ><a href="#ndtensorsjl" class=header-anchor >NDTensors.jl</a></h3> <p><a href="https://github.com/ITensor/ITensors.jl/tree/main/NDTensors">NDTensors.jl <i class="fab fa-github"></i></a> is the more traditional tensor algebra package underlying <code>ITensors.jl</code>. It defines an n-dimensional tensor <code>Tensor</code> that can have a variety of storage data types for various sparse and constrained tensors, such as dense, block sparse, and diagonal, with more planned such as tensors with isometric/unitary constraints. It implements high performance operations between mixtures of different tensor types such as addition, permutation, matrix factorization, and contraction. Additionally, it supports block sparse multithreaded contraction.</p> <p><a href="https://github.com/ITensor/ITensors.jl/tree/main/NDTensors"><i class="fab fa-github"></i> Source code</a></p> <h2>NDTensors.jl wish list</h2> <p><i class="fas fa-tools"></i> <strong>Tensors with isometric/unitary constraints:</strong> A special tensor storage type representing a tensor with isometric/unitary constraints would be useful in a variety of applications, such as isometrically constrained gradient optimization, automated simplification of tensor network contractions involving contractions of isometric tensors, etc. Please <a href="/about/#online_presence">reach out</a> if you are interested in helping us implement this feature.</p> <p><i class="fas fa-tools"></i> <strong>Lazy complex conjugation:</strong> It would be helpful for improving performance and memory usage to add support for lazy complex conjugation. For example, tensor contractions involving complex conjugation could be mapped directly to matrix multiplication calls to BLAS without allocating temporary complex conjugated tensors.</p> <hr /> <h3 id=itensorvisualizationbasejl ><a href="#itensorvisualizationbasejl" class=header-anchor >ITensorVisualizationBase.jl</a></h3> <p><a href="https://github.com/ITensor/ITensors.jl/tree/main/ITensorVisualizationBase">ITensorVisualizationBase.jl <i class="fab fa-github"></i></a> is an interface package for defining common functionality for different visualization backends for ITensors.jl. Currently, the main concrete backends are <a href="/projects/ITensorGLMakie.jl">ITensorGLMakie.jl</a> and <a href="/projects/ITensorUnicodePlots.jl">ITensorUnicodePlots.jl</a>. You should load one of those to visualization tensor networks of ITensors &#40;either as interactive plots or plots printing to the command line with unicode&#41;.</p> <p><a href="https://github.com/ITensor/ITensors.jl/tree/main/ITensorVisualizationBase"><i class="fab fa-github"></i> Source code</a></p> <hr /> <h3 id=itensorglmakiejl ><a href="#itensorglmakiejl" class=header-anchor >ITensorGLMakie.jl</a></h3> <p><a href="https://github.com/ITensor/ITensors.jl/tree/main/ITensorGLMakie">ITensorGLMakie.jl <i class="fab fa-github"></i></a> is a package I wrote for easily making interactive visualizations of tensor networks written with ITensors.jl, based on <a href="https://github.com/JuliaPlots/GraphMakie.jl">GraphMakie.jl <i class="fab fa-github"></i></a> and <a href="https://github.com/JuliaPlots/Makie.jl">Makie.jl <i class="fab fa-github"></i></a>. It supports clicking and dragging nodes/tensors of the tensor network.</p> <p><a href="https://github.com/ITensor/ITensors.jl/tree/main/ITensorGLMakie"><i class="fab fa-github"></i> Source code</a></p> <hr /> <p><i class="fas fa-tools"></i> <strong>More interactive customization:</strong> Currently, <code>ITensorGLMakie.jl</code> only support simple interactivity, such as clicking and dragging the nodes/tensors of the tensor network diagram. We would like to have more interactivity, such as interactively selecting the color, shape, and labels of the nodes/tensors.</p> <p><i class="fas fa-tools"></i> <strong>Multigraph visualization:</strong> <code>ITensorGLMakie.jl</code> currently visualizes tensors with multiple shared indices with a single edge and a label with information about the multiple edges. It would be helpful to directly visualize the multiple edges/indices. <code>GraphMakie.jl</code>, the package we use as a backend for <code>ITensorGLMakie</code>, implicitly <a href="https://github.com/JuliaPlots/GraphMakie.jl/issues/52">supports visualizing multigraphs</a> , so support for this should be straightforward to add.</p> <h3 id=itensorunicodeplotsjl ><a href="#itensorunicodeplotsjl" class=header-anchor >ITensorUnicodePlots.jl</a></h3> <p><a href="https://github.com/ITensor/ITensors.jl/tree/main/ITensorUnicodePlots">ITensorUnicodePlots.jl <i class="fab fa-github"></i></a> is an alternative backend for visualizing networks of ITensors as text output, based on <a href="https://github.com/JuliaPlots/UnicodePlots.jl">UnicodePlots.jl <i class="fab fa-github"></i></a>.</p> <p><a href="https://github.com/ITensor/ITensors.jl/tree/main/ITensorUnicodePlots"><i class="fab fa-github"></i> Source code</a></p> <h3 id=itensorgaussianmpsjl ><a href="#itensorgaussianmpsjl" class=header-anchor >ITensorGaussianMPS.jl</a></h3> <p><a href="https://github.com/ITensor/ITensors.jl/tree/main/ITensorGaussianMPS">ITensorGaussianMPS.jl <i class="fab fa-github"></i></a> is a Julia package I wrote for transforming free fermion states into tensor network states, based on an algorithm I developed during my Ph.D. with <a href="https://faculty.sites.uci.edu/dmrg/">Steven White <i class="fas fa-external-link-alt"></i></a>.</p> <p><a href="https://github.com/ITensor/ITensors.jl/tree/main/ITensorGaussianMPS"><i class="fab fa-github"></i> Source code</a></p> <p><i class="fas fa-file-alt"></i> <span class=bibref ><a href="#fishman2015">Fishman et al. (2015)</a></span></p> <h3 id=itensorinfinitempsjl ><a href="#itensorinfinitempsjl" class=header-anchor >ITensorInfiniteMPS.jl</a></h3> <p><a href="https://github.com/ITensor/ITensorInfiniteMPS.jl">ITensorInfiniteMPS.jl <i class="fab fa-github"></i></a> is a Julia package I wrote for extending the functionality of ITensors.jl to infinite MPS.</p> <p><a href="https://github.com/ITensor/ITensorInfiniteMPS.jl"><i class="fab fa-github"></i> Source code</a></p> <p><i class="fas fa-file-alt"></i> <span class=bibref ><a href="#stauber2017">Zauner-Stauber et al. (2017)</a></span></p> <h3 id=itensorgpujl ><a href="#itensorgpujl" class=header-anchor >ITensorGPU.jl</a></h3> <p><a href="https://github.com/ITensor/ITensors.jl/tree/main/ITensorGPU">ITensorGPU.jl <i class="fab fa-github"></i></a> is a a GPU backend for ITensors.jl primarily written by <a href="https://github.com/kshyatt">Katie Hyatt <i class="fas fa-external-link-alt"></i></a> while she was a postdoc at the CCQ.</p> <p><a href="https://github.com/ITensor/ITensors.jl/tree/main/ITensorGPU"><i class="fab fa-github"></i> Source code</a></p> <h3 id=itensornetworkadjl ><a href="#itensornetworkadjl" class=header-anchor >ITensorNetworkAD.jl</a></h3> <p><a href="https://github.com/ITensor/ITensorNetworkAD.jl">ITensorNetworkAD.jl <i class="fab fa-github"></i></a> is an experimental tensor network automatic differentiation &#40;AD&#41; library based on <a href="https://github.com/ITensor/ITensors.jl">ITensors.jl <i class="fab fa-github"></i></a>, started by <a href="https://linjianma.github.io">Linjian Ma <i class="fas fa-external-link-alt"></i></a> while he was a summer intern at the CCQ. It makes use of <a href="https://github.com/LinjianMa/AutoHOOT">AutoHOOT <i class="fab fa-github"></i></a>, a Python library for symbolic derivatives and simplifications of tensor networks.</p> <p><a href="https://github.com/ITensor/ITensorNetworkAD.jl"><i class="fab fa-github"></i> Source code</a></p> <h3 id=other_extensions_to_itensorsjl ><a href="#other_extensions_to_itensorsjl" class=header-anchor >Other extensions to ITensors.jl</a></h3> <p>Many packages are in development that extend the functionality of ITensors.jl, such as packages for performing network level contractions and gradient optimizations of tensor networks, packages for interfacing with quantum chemistry libraries like PySCF, and more. Stay tuned and keep an eye out on my <a href="https://github.com/mtfishman">Github page <i class="fab fa-github"></i></a>, the <a href="https://github.com/ITensor">ITensor Github organization <i class="fab fa-github"></i></a>, and the <a href="https://itensor.org">ITensor website <i class="fas fa-external-link-alt"></i></a>&#33;</p> <h2 id=pastaqjl ><a href="#pastaqjl" class=header-anchor >PastaQ.jl</a></h2> <p><a href="https://github.com/GTorlai/PastaQ.jl">PastaQ.jl <i class="fab fa-github"></i></a> is a package I co-develop with <a href="https://github.com/GTorlai">Giacomo Torlai <i class="fas fa-external-link-alt"></i></a> for simulating and analyzing quantum computers, including noisy state and process simulation with customizable noise models, state-of-the-art algorithms for tomography and ongoing work using automatic differentiation to optimize quantum circuits for implementing algorithms like variational quantum eigensolver &#40;VQE&#41; and optimal control.</p> <p><a href="https://pastaq.org"><i class="fas fa-external-link-alt"></i> pastaq.org</a></p> <p><a href="https://github.com/GTorlai/PastaQ.jl"><i class="fab fa-github"></i> Source code</a></p> <h2 id=itensor_c ><a href="#itensor_c" class=header-anchor >ITensor &#40;C&#43;&#43;&#41;</a></h2> <p><a href="https://github.com/ITensor/ITensor">ITensor <i class="fab fa-github"></i></a> is a C&#43;&#43; library for developing and performing tensor network calculations. I was the lead developer of <a href="https://itensor.org/news.html">C&#43;&#43; ITensor Version 3 <i class="fas fa-external-link-alt"></i></a>, the latest major release of the library which had many improvements to the interface and performance of block sparse calculations, including the introduction of block sparse multithreading with OpenMP.</p> <p><a href="https://itensor.org"><i class="fas fa-external-link-alt"></i> itensor.org</a></p> <p><a href="https://github.com/ITensor/ITensor"><i class="fab fa-github"></i> Source code</a></p> <h2 id=miscellaneous_julia_packages ><a href="#miscellaneous_julia_packages" class=header-anchor >Miscellaneous Julia Packages</a></h2> <h3 id=observersjl ><a href="#observersjl" class=header-anchor >Observers.jl</a></h3> <p>I co-developed <a href="https://github.com/GTorlai/Observers.jl">Observers.jl <i class="fab fa-github"></i></a> with <a href="https://github.com/GTorlai">Giacomo Torlai <i class="fas fa-external-link-alt"></i></a>. It is a package for conveniently specifying a set of measurements you want to make inside of an iterative method. It is currently being used in <a href="https://github.com/GTorlai/PastaQ.jl">PastaQ.jl <i class="fab fa-github"></i></a> inside iterative optimization methods like quantum state and process tomography as well as quantum circuit evolution, and we plan to make use of it in <a href="https://github.com/ITensor/ITensors.jl">ITensors.jl <i class="fab fa-github"></i></a>.</p> <p><i class="fas fa-tools"></i> <strong>Using <code>Observers.jl</code> in <code>ITensors.jl</code>:</strong> We are interested in using <a href="https://github.com/GTorlai/Observers.jl">Observers.jl <i class="fab fa-github"></i></a> inside iterative methods in <a href="https://github.com/ITensor/ITensors.jl">ITensors.jl <i class="fab fa-github"></i></a> like the density matrix renormalization group &#40;DMRG&#41; eigensolver as well as our circuit simulation functionality &#40;<code>apply</code>&#41;. Please <a href="/about/#online_presence">reach out</a> to me if you are interested in helping out with this&#33; It would be a good project for a new user trying to learn about DMRG, Julia, and ITensors.jl.</p> <p><a href="https://github.com/GTorlai/Observers.jl"><i class="fab fa-github"></i> Source code</a></p> <h3 id=serializedelementarraysjl ><a href="#serializedelementarraysjl" class=header-anchor >SerializedElementArrays.jl</a></h3> <p><a href="https://github.com/ITensor/SerializedElementArrays.jl">SerializedElementArrays.jl <i class="fab fa-github"></i></a> is a package I wrote that provides a new Julia Array type &#40;a SerializedElementArray&#41; whose elements are saved to disk. This can help in cases where you have collections of large contiguous data &#40;like an Array of very large Arrays&#41; which individually fit in memory but collectively do not. This is used for the write-to-disk feature in ITensors.jl.</p> <h2 id=tensor_network_algorithm_development ><a href="#tensor_network_algorithm_development" class=header-anchor >Tensor Network algorithm development</a></h2> <h3 id=free_fermion_tensor_networks ><a href="#free_fermion_tensor_networks" class=header-anchor >Free Fermion Tensor Networks</a></h3> <p><a href="https://faculty.sites.uci.edu/dmrg/">Steven White <i class="fas fa-external-link-alt"></i></a> and I developed an algorithm for obtaining a compact quantum circuit of local gates for a free fermion state. This leads to a straightforward way to construct tensor network state like matrix product states &#40;MPS&#41;, tree tensor networks &#40;TTN&#41;, and multi-scale entanglement renormalization ansatz &#40;MERA&#41; for free fermion states.</p> <p><i class="fas fa-file-alt"></i> <span class=bibref ><a href="#fishman2015">Fishman et al. (2015)</a></span></p> <h3 id=variational_uniform_matrix_product_states_and_tree_tensor_networks ><a href="#variational_uniform_matrix_product_states_and_tree_tensor_networks" class=header-anchor >Variational Uniform Matrix Product States and Tree Tensor Networks</a></h3> <p>In collaboration with colleagues at the University of Ghent and the University of Vienna, I helped to develop a new algorithm for finding ground states of quasi-1D quantum systems directly in the thermodynamic limit, which is faster at finding ground states than the state-of-the-art alternatives. The algorithm is called the variational uniform matrix product state &#40;VUMPS&#41; algorithm.</p> <p><i class="fas fa-file-alt"></i> <span class=bibref ><a href="#stauber2017">Zauner-Stauber et al. (2017)</a></span></p> <p>In collaboration with colleagues at the CCQ, I worked on extending the VUMPS algorithm to solve for ground states of infinite tree tensor networks states, such as states on the Bethe lattices, in an algorithm we called the variational uniform tree state &#40;VUTS&#41; algorithm.</p> <p><i class="fas fa-file-alt"></i> <span class=bibref ><a href="#lunts2020">Lunts et al. (2020)</a></span></p> <h3 id=improved_contraction_methods_for_infinite_2d_tensor_networks ><a href="#improved_contraction_methods_for_infinite_2d_tensor_networks" class=header-anchor >Improved Contraction Methods for Infinite 2D Tensor Networks</a></h3> <p>In collaboration with colleagues at the University of Ghent and the University of Vienna, I worked on extending the VUMPS algorithm to the problem of contracting infinite 2D tensor networks and showed that in many cases it outperforms the standard method, the corner transfer matrix renormalization group &#40;CTMRG&#41; algorithm. In addition, I worked on a fixed point formulation of CTMRG which we also showed was faster than the original CTMRG algorithm, which we called the fixed point corner method &#40;FPCM&#41;.</p> <p><i class="fas fa-file-alt"></i> <span class=bibref ><a href="#fishman2017">Fishman et al. (2017)</a></span></p> <h3 id=easing_the_sign_problem_with_variational_circuits_and_automatic_differentation ><a href="#easing_the_sign_problem_with_variational_circuits_and_automatic_differentation" class=header-anchor >Easing the Sign Problem with Variational Circuits and Automatic Differentation</a></h3> <p>With colleagues from CCQ and other institutions, I helped develop a method for decreasing the average sign of a wavefunction by optimizing a quantum circuit ansatz with automatic differentiation. This could have implications for improving the performance of monte carlo algorithms.</p> <p><i class="fas fa-file-alt"></i> <span class=bibref ><a href="#torlai2019">Torlai et al. (2019)</a></span></p> <h2 id=references ><a href="#references" class=header-anchor >References</a></h2> <p><i class="fas fa-file-alt"></i> <a id=fishman2015  class=anchor ></a><strong>Fishman, White</strong> <a href="https://arxiv.org/abs/1504.07701">Compression of Correlation Matrices and an Efficient Method for Forming Matrix Product States of Fermionic Gaussian States</a>, 2015.</p> <p><i class="fas fa-file-alt"></i> <a id=stauber2017  class=anchor ></a><strong>Zauner-Stauber, Vanderstraeten, Fishman, Verstraete, Haegeman</strong> <a href="https://arxiv.org/abs/1701.07035">Variational optimization algorithms for uniform matrix product states</a>, 2017.</p> <p><i class="fas fa-file-alt"></i> <a id=fishman2017  class=anchor ></a><strong>Fishman, Vanderstraeten, Zauner-Stauber, Haegeman, Verstraete</strong> <a href="https://arxiv.org/abs/1711.05881">Faster Methods for Contracting Infinite 2D Tensor Networks</a>, 2017.</p> <p><i class="fas fa-file-alt"></i> <a id=lunts2020  class=anchor ></a><strong>Lunts, George, Stoudenmire, Fishman</strong> <a href="https://arxiv.org/abs/2010.06543">The Hubbard model on the Bethe lattice via variational uniform tree states: metal-insulator transition and a Fermi liquid</a>, 2020.</p> <p><i class="fas fa-file-alt"></i> <a id=fishman2020  class=anchor ></a><strong>Fishman, White, Stoudenmire</strong> <a href="https://arxiv.org/abs/2007.14822">The ITensor Software Library for Tensor Network Calculations</a>, 2020.</p> <p><i class="fas fa-file-alt"></i> <a id=torlai2019  class=anchor ></a><strong>Torlai, Carrasquilla, Fishman, Melko, Fisher</strong> <a href="https://arxiv.org/abs/1906.04654">Wavefunction positivization via automatic differentiation</a>, 2019.</p> <div class=page-foot > <div class=copyright > &copy; Matthew Fishman. Last modified: March 03, 2023. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. View the source code for this website on <a href="https://github.com/mtfishman/mtfishman.github.io">Github</a>. </div> </div> </div>